{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174f958c-e8be-4d7e-99ee-37c0149631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import medfilt\n",
    "import pywt\n",
    "from ecgdetectors import Detectors\n",
    "from scipy.signal import resample_poly\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b9013b-cfce-441e-bc7d-f575c8b3c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsr_data = ['./data/nsrdb/16265',\n",
    " './data/nsrdb/16272',\n",
    " './data/nsrdb/16273',\n",
    " './data/nsrdb/16420',\n",
    " './data/nsrdb/16483',\n",
    " './data/nsrdb/16539',\n",
    " './data/nsrdb/16773',\n",
    " './data/nsrdb/16786',\n",
    " './data/nsrdb/16795',\n",
    " './data/nsrdb/17052',\n",
    " './data/nsrdb/17453',\n",
    " './data/nsrdb/18177',\n",
    " './data/nsrdb/18184',\n",
    " './data/nsrdb/19088',\n",
    " './data/nsrdb/19090',\n",
    " './data/nsrdb/19093',\n",
    " './data/nsrdb/19140',\n",
    " './data/nsrdb/19830']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e25bde-64ef-4ff7-b4d9-3d9d1cd489c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_60min_and_segment(record_paths):\n",
    "    \"\"\"\n",
    "    Given a list of NSR record paths (e.g. './data/nsrdb/16265'),\n",
    "    1) Read the first 60 minutes of the ECG from each record\n",
    "    2) Segment that 60-min signal into six 10-min parts\n",
    "    3) Return a dictionary mapping record_name -> [segment1, segment2, ... segment6]\n",
    "       Each segment is a NumPy array of shape (num_samples_5min, num_channels).\n",
    "    \"\"\"\n",
    "    \n",
    "    # For 60 min, we have 60 * 60 = 3600 seconds. \n",
    "    # For 10 min, we have 10 * 60 = 600 seconds.\n",
    "    \n",
    "    first_60min_segments = {}\n",
    "    \n",
    "    for record_path in record_paths:\n",
    "        \n",
    "        # Extract record name from path\n",
    "        # e.g. record_path = \"./data/nsrdb/16265\" => record_name = \"16265\"\n",
    "        record_dir, record_name = os.path.split(record_path)\n",
    "      \n",
    "        print(f\"Processing {record_name} ...\")\n",
    "        \n",
    "        # We want the first 60 minutes => 3600 seconds => num_samples = 3600 * fs\n",
    "        fs = 128\n",
    "        num_samples_60min = int(60 * 60 * fs)\n",
    "        \n",
    "        # Read from sample 0 to sample 0+num_samples_30min\n",
    "        try:\n",
    "            rec = wfdb.rdrecord(record_path, sampfrom=0, sampto=num_samples_60min)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not read {record_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if rec.p_signal is None:\n",
    "            print(f\"[WARN] No signal found in {record_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_60min = rec.p_signal[:,0]\n",
    "        \n",
    "        # Segment the 60-min array into six 10-min parts\n",
    "        # Each 10-min part = 10 * 60 * fs samples\n",
    "        \n",
    "        samples_10min = int(10 * 60 * fs)  # 600 seconds * 128 => 38400 * 2\n",
    "        \n",
    "        # We can slice in 6 equal blocks\n",
    "        segments_10min = []\n",
    "        for i in range(6):\n",
    "            start_i = i * samples_10min\n",
    "            end_i = start_i + samples_10min\n",
    "            segment = full_60min[start_i:end_i]\n",
    "            segments_10min.append(segment)\n",
    "        \n",
    "        # Store in a dictionary\n",
    "        first_60min_segments[record_name] = segments_10min\n",
    "        print(f\"[OK] Extracted 6 segments of 10 min each from {record_name}.\")\n",
    "    \n",
    "    return first_60min_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973542eb-156a-4d2c-877b-cd034d08cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal(X, dwt_transform, dlevels, cutoff_low, cutoff_high):\n",
    "    coeffs = pywt.wavedec(X, dwt_transform, level=dlevels)   # wavelet transform 'bior4.4'\n",
    "    # scale 0 to cutoff_low \n",
    "    for ca in range(0,cutoff_low):\n",
    "        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n",
    "    # scale cutoff_high to end\n",
    "    for ca in range(cutoff_high, len(coeffs)):\n",
    "        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n",
    "    Y = pywt.waverec(coeffs, dwt_transform) # inverse wavelet transform\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b235591-7fd2-408b-98dc-8388269ff90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_peak_finder(ecg_sig):\n",
    "    BASIC_SRATE = 128\n",
    "    signal_pad_samples = 10\n",
    "    signal_pad = np.zeros(signal_pad_samples)  # Pad to help detect early peaks\n",
    "    scd_30_denoised_ = ...  # Your denoised 60-min ECG segment\n",
    "    \n",
    "    # Initialize the detectors at the given sampling rate\n",
    "    detector_obj = Detectors(BASIC_SRATE)\n",
    "    \n",
    "    # Dictionary of detector functions\n",
    "    detectors = {\n",
    "        'pan_tompkins_detector': detector_obj.pan_tompkins_detector,\n",
    "        'hamilton_detector': detector_obj.hamilton_detector,\n",
    "        'christov_detector': detector_obj.christov_detector,\n",
    "        'engzee_detector': detector_obj.engzee_detector,\n",
    "        'swt_detector': detector_obj.swt_detector,\n",
    "        'two_average_detector': detector_obj.two_average_detector,\n",
    "    }\n",
    "    \n",
    "    r_peaks = np.array(detector_obj.christov_detector(np.hstack((signal_pad, ecg_sig)) )) - signal_pad_samples\n",
    "    return r_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231af9ab-5049-4926-adc3-dbbb52aaaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hrv_features(r_peaks, fs=128):\n",
    "    \"\"\"\n",
    "    Time-domain HRV features from R-peaks.\n",
    "    Returns a dict with:\n",
    "      MeanRR, RMSDD, pNN50, SDRR, CVRR, NN50, MinRR, MaxRR\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'MeanRR': 0.0,\n",
    "        'RMSDD': 0.0,\n",
    "        'MADRR': 0.0,\n",
    "        'MCVNN': 0.0,\n",
    "        'pNN20': 0.0,\n",
    "        'pNN50': 0.0,\n",
    "        'SDRR': 0.0,\n",
    "        'CVRR': 0.0,\n",
    "        'NN20': 0,\n",
    "        'NN50': 0,\n",
    "        'MinRR': 0.0,\n",
    "        'MaxRR': 0.0\n",
    "    }\n",
    "\n",
    "    rr_samples = np.diff(r_peaks)\n",
    "    rr_ms = (rr_samples / fs) * 1000.0  # convert to ms\n",
    "\n",
    "    mean_rr = np.mean(rr_ms)\n",
    "    sdrr = np.std(rr_ms, ddof=1) if len(rr_ms) > 1 else 0.0\n",
    "    min_rr = np.min(rr_ms)\n",
    "    max_rr = np.max(rr_ms)\n",
    "\n",
    "    rr_diffs = np.diff(rr_ms)\n",
    "    rmssd = np.sqrt(np.mean(rr_diffs**2)) if len(rr_diffs) > 0 else 0.0\n",
    "    nn50 = np.sum(np.abs(rr_diffs) > 50)\n",
    "    pnn50 = (nn50 / len(rr_diffs)) * 100 if len(rr_diffs) > 0 else 0.0\n",
    "    nn20 = np.sum(np.abs(rr_diffs) > 20)\n",
    "    pnn20 = (nn20 / len(rr_diffs)) * 100 if len(rr_diffs) > 0 else 0.0\n",
    "    cvrr = (sdrr / mean_rr * 100.0) if mean_rr else 0.0\n",
    "    madrr = np.median(np.abs(rr_ms - np.median(rr_ms)))\n",
    "    mcvnn = np.abs(rr_samples).mean()\n",
    "\n",
    "    features['MeanRR'] = mean_rr / 1000.0\n",
    "    features['RMSDD'] = rmssd / 1000.0\n",
    "    # new \n",
    "    features['MADRR'] = madrr / 1000.0\n",
    "    features['MCVNN'] = mcvnn / 1000.0 \n",
    "    features['pNN20'] = pnn20 / 1000.0\n",
    "    \n",
    "    \n",
    "    features['pNN50'] = pnn50 / 1000.0\n",
    "    features['SDRR'] = sdrr / 1000.0\n",
    "    features['CVRR'] = cvrr / 1000.0 \n",
    "    # new\n",
    "    features['NN20'] = nn20 / 1000.0\n",
    "    \n",
    "    features['NN50'] = nn50 / 1000.0\n",
    "    features['MinRR'] = min_rr / 1000.0 \n",
    "    features['MaxRR'] = max_rr / 1000.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a8f538-3098-4d9b-8a99-e413a11f35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = [\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\", \"Sixth\"]\n",
    "\n",
    "def save_hrv_to_csv(features_dict, csv_path):\n",
    "    \"\"\"\n",
    "    Save a single row of features_dict into a CSV at csv_path.\n",
    "    Overwrites if file exists.\n",
    "    Columns: [MeanRR, RMSDD, MADRR, MCVNN, pNN20, pNN50, SDRR, CVRR, NN20, NN50, MinRR, MaxRR]\n",
    "    \"\"\"\n",
    "    columns = [\"MeanRR\", \"RMSDD\", \"MADRR\", \"MCVNN\", \"pNN20\", \"pNN50\", \"SDRR\", \"CVRR\", \"NN20\", \"NN50\", \"MinRR\", \"MaxRR\"]\n",
    "    with open(csv_path, mode='w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer.writerow({col: features_dict[col] for col in columns})\n",
    "    print(f\"  -> Saved HRV features to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d89fba-f11e-4d06-a7b8-fb2dd5f1a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 128\n",
    "def get_median_filter_width(sampling_rate, duration):\n",
    "    res = int( sampling_rate*duration )\n",
    "    res += ((res%2) - 1) # needs to be an odd number\n",
    "    return res\n",
    "\n",
    "ms_flt_array = [0.2,0.6]    #<-- length of baseline fitting filters (in seconds)\n",
    "mfa = np.zeros(len(ms_flt_array), dtype='int')\n",
    "for i in range(0, len(ms_flt_array)):\n",
    "    mfa[i] = get_median_filter_width(fs,ms_flt_array[i])\n",
    "\n",
    "def filter_signal(X):\n",
    "    global mfa\n",
    "    X0 = X  #read orignal signal\n",
    "    for mi in range(0,len(mfa)):\n",
    "        X0 = medfilt(X0,mfa[mi]) # apply median filter one by one on top of each other\n",
    "    X0 = np.subtract(X,X0)  # finally subtract from orignal signal\n",
    "    return X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621767ed-7c21-4ace-aec2-cb6c2bc22070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 16265 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16265.\n",
      "Processing 16272 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16272.\n",
      "Processing 16273 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16273.\n",
      "Processing 16420 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16420.\n",
      "Processing 16483 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16483.\n",
      "Processing 16539 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16539.\n",
      "Processing 16773 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16773.\n",
      "Processing 16786 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16786.\n",
      "Processing 16795 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 16795.\n",
      "Processing 17052 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 17052.\n",
      "Processing 17453 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 17453.\n",
      "Processing 18177 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 18177.\n",
      "Processing 18184 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 18184.\n",
      "Processing 19088 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 19088.\n",
      "Processing 19090 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 19090.\n",
      "Processing 19093 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 19093.\n",
      "Processing 19140 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 19140.\n",
      "Processing 19830 ...\n",
      "[OK] Extracted 6 segments of 10 min each from 19830.\n"
     ]
    }
   ],
   "source": [
    "nsr_segments  = extract_first_60min_and_segment(nsr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5543e1b-745b-445c-9cca-97ebabb8e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_nsr = \"NSR_Features_CSV_1h_10min_segments\"\n",
    "os.makedirs(output_dir_nsr, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4c7e2d-3bc5-47f5-85b8-96f98496638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NSR subject 16265...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16265_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16265_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16265_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16265_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16265_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16265_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16272...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16272_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16272_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16272_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16272_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16272_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16272_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16273...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16273_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16273_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16273_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16273_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16273_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16273_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16420...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16420_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16420_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16420_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16420_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16420_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16420_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16483...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16483_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16483_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16483_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16483_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16483_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16483_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16539...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16539_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16539_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16539_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16539_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16539_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16539_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16773...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16773_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16773_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16773_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16773_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16773_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16773_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16786...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16786_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16786_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16786_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16786_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16786_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16786_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 16795...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16795_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16795_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16795_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16795_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16795_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_16795_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 17052...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17052_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17052_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17052_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17052_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17052_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17052_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 17453...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17453_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17453_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17453_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17453_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17453_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_17453_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 18177...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18177_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18177_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18177_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18177_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18177_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18177_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 18184...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18184_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18184_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18184_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18184_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18184_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_18184_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 19088...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19088_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19088_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19088_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19088_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19088_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19088_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 19090...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19090_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19090_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19090_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19090_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19090_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19090_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 19093...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19093_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19093_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19093_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19093_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19093_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19093_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 19140...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19140_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19140_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19140_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19140_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19140_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19140_Sixth_10_min.csv\n",
      "\n",
      "Processing NSR subject 19830...\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19830_First_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19830_Second_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19830_Third_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19830_Fourth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19830_Fifth_10_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV_1h_10min_segments\\NSR_19830_Sixth_10_min.csv\n"
     ]
    }
   ],
   "source": [
    "# Process NSR data (normal order => 1st is earliest 10 min)\n",
    "for subj_id, seg_list in nsr_segments.items():\n",
    "    print(f\"\\nProcessing NSR subject {subj_id}...\")\n",
    "    # seg_list[0] = first 10 min, seg_list[1] = second 10 min, ...\n",
    "    for i in range(6):\n",
    "        segment_label = f\"NSR_{subj_id}_{label_map[i]}_10_min\"  # e.g. \"First_10_min\"\n",
    "        ecg_signal = seg_list[i]\n",
    "\n",
    "        flt_sig = filter_signal(ecg_signal)\n",
    "\n",
    "        denoised_signal = denoise_signal(flt_sig, 'bior4.4', 9, 1 , 7)\n",
    "        # 1) Detect R-peaks\n",
    "        r_peaks = r_peak_finder(denoised_signal)\n",
    "        # 2) Compute HRV\n",
    "        feats = compute_hrv_features(r_peaks, fs=128)\n",
    "        # 3) Build CSV file name, e.g. \"NSR_SubjectX_First_10_min.csv\"\n",
    "        csv_filename = f\"{segment_label}.csv\"\n",
    "        csv_path = os.path.join(output_dir_nsr, csv_filename)\n",
    "        # 4) Save\n",
    "        save_hrv_to_csv(feats, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2d273-5c99-4ac9-91b2-e4b9ec58c3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
