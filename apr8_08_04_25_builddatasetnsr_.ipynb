{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0d776f-479d-47ee-80e5-42e7f40c0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import medfilt\n",
    "import pywt\n",
    "from ecgdetectors import Detectors\n",
    "from scipy.signal import resample_poly\n",
    "import csv\n",
    "\n",
    "nsr_data = ['./data/nsrdb/16265',\n",
    " './data/nsrdb/16272',\n",
    " './data/nsrdb/16273',\n",
    " './data/nsrdb/16420',\n",
    " './data/nsrdb/16483',\n",
    " './data/nsrdb/16539',\n",
    " './data/nsrdb/16773',\n",
    " './data/nsrdb/16786',\n",
    " './data/nsrdb/16795',\n",
    " './data/nsrdb/17052',\n",
    " './data/nsrdb/17453',\n",
    " './data/nsrdb/18177',\n",
    " './data/nsrdb/18184',\n",
    " './data/nsrdb/19088',\n",
    " './data/nsrdb/19090',\n",
    " './data/nsrdb/19093',\n",
    " './data/nsrdb/19140',\n",
    " './data/nsrdb/19830']\n",
    "\n",
    "def extract_first_30min_and_segment(record_paths):\n",
    "    \"\"\"\n",
    "    Given a list of NSR record paths (e.g. './data/nsrdb/16265'),\n",
    "    1) Read the first 30 minutes of the ECG from each record\n",
    "    2) Segment that 30-min signal into six 5-min parts\n",
    "    3) Return a dictionary mapping record_name -> [segment1, segment2, ... segment6]\n",
    "       Each segment is a NumPy array of shape (num_samples_5min, num_channels).\n",
    "    \"\"\"\n",
    "    \n",
    "    # For 30 min, we have 30 * 60 = 1800 seconds. \n",
    "    # For 5 min, we have 5 * 60 = 300 seconds.\n",
    "    \n",
    "    first_30min_segments = {}\n",
    "    \n",
    "    for record_path in record_paths:\n",
    "        \n",
    "        # Extract record name from path\n",
    "        # e.g. record_path = \"./data/nsrdb/16265\" => record_name = \"16265\"\n",
    "        record_dir, record_name = os.path.split(record_path)\n",
    "      \n",
    "        print(f\"Processing {record_name} ...\")\n",
    "        \n",
    "        # We want the first 30 minutes => 1800 seconds => num_samples = 1800 * fs\n",
    "        fs = 128\n",
    "        num_samples_30min = int(30 * 60 * fs)\n",
    "        \n",
    "        # Read from sample 0 to sample 0+num_samples_30min\n",
    "        try:\n",
    "            rec = wfdb.rdrecord(record_path, sampfrom=0, sampto=num_samples_30min)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not read {record_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if rec.p_signal is None:\n",
    "            print(f\"[WARN] No signal found in {record_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        full_30min = rec.p_signal[:,0]\n",
    "        \n",
    "        # Segment the 30-min array into six 5-min parts\n",
    "        # Each 5-min part = 5 * 60 * fs samples\n",
    "        \n",
    "        samples_5min = int(5 * 60 * fs)  # 300 seconds * 128 => 38400\n",
    "        \n",
    "        # We can slice in 6 equal blocks\n",
    "        segments_5min = []\n",
    "        for i in range(6):\n",
    "            start_i = i * samples_5min\n",
    "            end_i = start_i + samples_5min\n",
    "            segment = full_30min[start_i:end_i]\n",
    "            segments_5min.append(segment)\n",
    "        \n",
    "        # Store in a dictionary\n",
    "        first_30min_segments[record_name] = segments_5min\n",
    "        print(f\"[OK] Extracted 6 segments of 5 min each from {record_name}.\")\n",
    "    \n",
    "    return first_30min_segments\n",
    "\n",
    "\n",
    "def denoise_signal(X, dwt_transform, dlevels, cutoff_low, cutoff_high):\n",
    "    coeffs = pywt.wavedec(X, dwt_transform, level=dlevels)   # wavelet transform 'bior4.4'\n",
    "    # scale 0 to cutoff_low \n",
    "    for ca in range(0,cutoff_low):\n",
    "        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n",
    "    # scale cutoff_high to end\n",
    "    for ca in range(cutoff_high, len(coeffs)):\n",
    "        coeffs[ca]=np.multiply(coeffs[ca],[0.0])\n",
    "    Y = pywt.waverec(coeffs, dwt_transform) # inverse wavelet transform\n",
    "    return Y  \n",
    "\n",
    "\n",
    "def r_peak_finder(ecg_sig):\n",
    "    BASIC_SRATE = 128\n",
    "    signal_pad_samples = 10\n",
    "    signal_pad = np.zeros(signal_pad_samples)  # Pad to help detect early peaks\n",
    "    scd_30_denoised_ = ...  # Your denoised 30-min ECG segment\n",
    "    \n",
    "    # Initialize the detectors at the given sampling rate\n",
    "    detector_obj = Detectors(BASIC_SRATE)\n",
    "    \n",
    "    # Dictionary of detector functions\n",
    "    detectors = {\n",
    "        'pan_tompkins_detector': detector_obj.pan_tompkins_detector,\n",
    "        'hamilton_detector': detector_obj.hamilton_detector,\n",
    "        'christov_detector': detector_obj.christov_detector,\n",
    "        'engzee_detector': detector_obj.engzee_detector,\n",
    "        'swt_detector': detector_obj.swt_detector,\n",
    "        'two_average_detector': detector_obj.two_average_detector,\n",
    "    }\n",
    "    \n",
    "    r_peaks = np.array(detector_obj.engzee_detector(np.hstack((signal_pad, ecg_sig)) )) - signal_pad_samples\n",
    "    return r_peaks\n",
    "\n",
    "\n",
    "def compute_hrv_features(r_peaks, fs=128):\n",
    "    \"\"\"\n",
    "    Time-domain HRV features from R-peaks.\n",
    "    Returns a dict with:\n",
    "      MeanRR, RMSDD, pNN50, SDRR, CVRR, NN50, MinRR, MaxRR\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'MeanRR': 0.0,\n",
    "        'RMSDD': 0.0,\n",
    "        'pNN50': 0.0,\n",
    "        'SDRR': 0.0,\n",
    "        'CVRR': 0.0,\n",
    "        'NN50': 0,\n",
    "        'MinRR': 0.0,\n",
    "        'MaxRR': 0.0\n",
    "    }\n",
    "\n",
    "    rr_samples = np.diff(r_peaks)\n",
    "    rr_ms = (rr_samples / fs) * 1000.0  # convert to ms\n",
    "\n",
    "    mean_rr = np.mean(rr_ms)\n",
    "    sdrr = np.std(rr_ms, ddof=1) if len(rr_ms) > 1 else 0.0\n",
    "    min_rr = np.min(rr_ms)\n",
    "    max_rr = np.max(rr_ms)\n",
    "\n",
    "    rr_diffs = np.diff(rr_ms)\n",
    "    rmssd = np.sqrt(np.mean(rr_diffs**2)) if len(rr_diffs) > 0 else 0.0\n",
    "    nn50 = np.sum(np.abs(rr_diffs) > 50)\n",
    "    pnn50 = (nn50 / len(rr_diffs)) * 100 if len(rr_diffs) > 0 else 0.0\n",
    "\n",
    "    cvrr = (sdrr / mean_rr * 100.0) if mean_rr else 0.0\n",
    "\n",
    "    features['MeanRR'] = mean_rr / 1000.0\n",
    "    features['RMSDD'] = rmssd / 1000.0\n",
    "    features['pNN50'] = pnn50 / 1000.0\n",
    "    features['SDRR'] = sdrr / 1000.0\n",
    "    features['CVRR'] = cvrr / 1000.0 \n",
    "    features['NN50'] = nn50 / 1000.0\n",
    "    features['MinRR'] = min_rr / 1000.0 \n",
    "    features['MaxRR'] = max_rr / 1000.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f80b82-34e4-4a23-a1b9-d4c643b74036",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = [\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\", \"Sixth\"]\n",
    "\n",
    "def save_hrv_to_csv(features_dict, csv_path):\n",
    "    \"\"\"\n",
    "    Save a single row of features_dict into a CSV at csv_path.\n",
    "    Overwrites if file exists.\n",
    "    Columns: [MeanRR, RMSDD, pNN50, SDRR, CVRR, NN50, MinRR, MaxRR]\n",
    "    \"\"\"\n",
    "    columns = [\"MeanRR\", \"RMSDD\", \"pNN50\", \"SDRR\", \"CVRR\", \"NN50\", \"MinRR\", \"MaxRR\"]\n",
    "    with open(csv_path, mode='w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer.writerow({col: features_dict[col] for col in columns})\n",
    "    print(f\"  -> Saved HRV features to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f293af-f29e-46cb-8f71-3cc6ded57c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb01253d-be5b-4025-9dab-934704bb3dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 16265 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16265.\n",
      "Processing 16272 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16272.\n",
      "Processing 16273 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16273.\n",
      "Processing 16420 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16420.\n",
      "Processing 16483 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16483.\n",
      "Processing 16539 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16539.\n",
      "Processing 16773 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16773.\n",
      "Processing 16786 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16786.\n",
      "Processing 16795 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 16795.\n",
      "Processing 17052 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 17052.\n",
      "Processing 17453 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 17453.\n",
      "Processing 18177 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 18177.\n",
      "Processing 18184 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 18184.\n",
      "Processing 19088 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 19088.\n",
      "Processing 19090 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 19090.\n",
      "Processing 19093 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 19093.\n",
      "Processing 19140 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 19140.\n",
      "Processing 19830 ...\n",
      "[OK] Extracted 6 segments of 5 min each from 19830.\n"
     ]
    }
   ],
   "source": [
    "nsr_segments  = extract_first_30min_and_segment(nsr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a160f6-8870-448b-bc54-5897cd54eeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16265': [array([-0.165, -0.155, -0.195, ..., -0.295, -0.135, -0.295]),\n",
       "  array([-0.285, -0.285, -0.185, ..., -0.365,  1.475,  2.795]),\n",
       "  array([ 2.935,  2.535,  0.605, ..., -0.195, -0.145, -0.165]),\n",
       "  array([-0.145, -0.115, -0.125, ..., -0.145, -0.145, -0.135]),\n",
       "  array([-0.175, -0.135, -0.175, ..., -0.135, -0.135, -0.105]),\n",
       "  array([-0.115, -0.125, -0.115, ..., -0.135, -0.175, -0.175])],\n",
       " '16272': [array([-0.185, -0.215, -0.215, ...,  0.055, -0.255, -0.205]),\n",
       "  array([-0.175, -0.205, -0.175, ..., -0.085, -0.075, -0.055]),\n",
       "  array([-0.085, -0.085, -0.095, ..., -0.095, -0.065, -0.005]),\n",
       "  array([ 0.575,  1.135,  0.475, ..., -0.075, -0.065, -0.065]),\n",
       "  array([-0.075, -0.075, -0.085, ...,  1.255, -0.115, -0.685]),\n",
       "  array([-0.475, -0.155, -0.085, ..., -0.045, -0.045, -0.055])],\n",
       " '16273': [array([-0.305, -0.265, -0.245, ..., -0.015, -0.005, -0.005]),\n",
       "  array([ 0.005,  0.035, -0.005, ...,  1.485,  2.755,  3.225]),\n",
       "  array([ 1.925,  0.335, -0.205, ..., -0.135, -0.135, -0.135]),\n",
       "  array([-0.135, -0.145, -0.155, ..., -0.165, -0.175, -0.175]),\n",
       "  array([-0.185, -0.185, -0.185, ..., -0.155, -0.155, -0.145]),\n",
       "  array([-0.145, -0.115, -0.105, ...,  0.485,  0.395,  0.365])],\n",
       " '16420': [array([-0.095, -0.085, -0.085, ..., -0.075, -0.075, -0.025]),\n",
       "  array([-0.025,  0.025,  0.055, ..., -0.115, -0.135, -0.135]),\n",
       "  array([-0.135, -0.135, -0.145, ..., -0.105, -0.085, -0.075]),\n",
       "  array([-0.095, -0.055, -0.055, ...,  0.025, -0.015, -0.015]),\n",
       "  array([-0.035, -0.065, -0.085, ..., -0.065, -0.035, -0.015]),\n",
       "  array([-0.025, -0.005,  0.015, ..., -0.075, -0.115, -0.095])],\n",
       " '16483': [array([-0.075, -0.165, -0.225, ..., -0.225, -0.245, -0.285]),\n",
       "  array([-0.285, -0.285, -0.275, ...,  0.315, -0.495, -0.645]),\n",
       "  array([-0.195, -0.125, -0.115, ..., -0.095, -0.255, -0.295]),\n",
       "  array([-0.315, -0.305, -0.325, ..., -0.195, -0.155, -0.185]),\n",
       "  array([-0.195, -0.205, -0.105, ..., -0.185, -0.205, -0.685]),\n",
       "  array([-0.615,  0.185,  1.365, ..., -0.045,  0.035,  0.165])],\n",
       " '16539': [array([-0.185, -0.155, -0.145, ..., -0.025, -0.005, -0.005]),\n",
       "  array([-0.005, -0.015,  0.005, ..., -0.065, -0.075, -0.095]),\n",
       "  array([-0.065, -0.095, -0.095, ...,  0.015, -0.025,  0.415]),\n",
       "  array([-0.375,  0.365, -0.395, ..., -0.265, -0.335, -0.335]),\n",
       "  array([-0.355, -0.265, -0.275, ..., -0.145, -0.175, -0.165]),\n",
       "  array([-0.155, -0.155, -0.145, ..., -0.115, -0.165, -0.125])],\n",
       " '16773': [array([-0.355, -0.355, -0.345, ...,  2.835,  2.365, -0.185]),\n",
       "  array([-1.005, -1.025, -0.835, ..., -0.265, -0.255, -0.295]),\n",
       "  array([-0.295, -0.295, -0.295, ...,  0.565,  0.345,  0.145]),\n",
       "  array([-0.025, -0.115, -0.165, ...,  2.255,  2.945,  2.705]),\n",
       "  array([ 0.195, -0.795, -0.755, ..., -0.255, -0.275, -0.265]),\n",
       "  array([-0.265, -0.255, -0.225, ..., -0.235, -0.235, -0.235])],\n",
       " '16786': [array([-0.275, -0.245, -0.285, ..., -0.145, -0.175, -0.195]),\n",
       "  array([-0.205, -0.195, -0.185, ..., -0.135, -0.115, -0.105]),\n",
       "  array([-0.075, -0.025, -0.045, ..., -0.165, -0.155, -0.145]),\n",
       "  array([-0.155, -0.165, -0.185, ..., -0.715, -0.345,  1.235]),\n",
       "  array([ 2.425,  2.945,  1.655, ..., -0.255, -0.225, -0.235]),\n",
       "  array([-0.225, -0.185, -0.355, ...,  0.305,  0.205,  0.075])],\n",
       " '16795': [array([ 0.125,  0.005,  0.025, ..., -0.245, -0.235, -0.225]),\n",
       "  array([-0.225, -0.245, -0.245, ..., -0.265, -0.285, -0.265]),\n",
       "  array([-0.145,  0.065,  0.345, ..., -0.125, -0.135, -0.125]),\n",
       "  array([-0.105, -0.105, -0.095, ..., -0.145, -0.155, -0.125]),\n",
       "  array([-0.115, -0.125, -0.095, ..., -0.165, -0.115, -0.095]),\n",
       "  array([-0.085, -0.095, -0.045, ..., -0.135, -0.155, -0.145])],\n",
       " '17052': [array([-0.035, -0.015, -0.045, ..., -0.165, -0.215, -0.175]),\n",
       "  array([-0.195, -0.195, -0.165, ..., -0.145, -0.195, -0.155]),\n",
       "  array([-0.085, -0.045, -0.125, ..., -0.095, -0.075, -0.085]),\n",
       "  array([-0.095, -0.095, -0.085, ..., -0.055, -0.045, -0.035]),\n",
       "  array([-0.025,  0.005,  0.255, ..., -0.015, -0.045, -0.015]),\n",
       "  array([ 0.185,  0.865,  1.535, ..., -0.145, -0.135, -0.125])],\n",
       " '17453': [array([0.525, 0.695, 0.475, ..., 0.005, 0.065, 0.095]),\n",
       "  array([ 0.035,  0.025, -0.015, ..., -0.075, -0.075, -0.045]),\n",
       "  array([-0.175,  0.025,  0.035, ..., -0.045, -0.055, -0.015]),\n",
       "  array([-0.005, -0.005, -0.045, ..., -0.145, -0.145, -0.135]),\n",
       "  array([-0.095, -0.105, -0.065, ..., -0.405, -0.275, -0.215]),\n",
       "  array([-0.205, -0.195, -0.205, ..., -0.195, -0.195, -0.185])],\n",
       " '18177': [array([-0.215, -0.195, -0.185, ..., -0.015, -0.115, -0.165]),\n",
       "  array([-0.215, -0.235, -0.235, ..., -0.105, -0.155, -0.155]),\n",
       "  array([-0.235, -0.305, -0.345, ...,  0.005, -0.005, -0.025]),\n",
       "  array([-0.055, -0.065, -0.075, ...,  0.105,  0.095,  0.105]),\n",
       "  array([ 0.085,  0.055, -0.025, ..., -0.135, -0.145, -0.125]),\n",
       "  array([-0.145, -0.135, -0.135, ..., -0.125, -0.035,  0.055])],\n",
       " '18184': [array([-0.325, -0.395, -0.255, ...,  0.035, -0.005, -0.025]),\n",
       "  array([-0.045, -0.065, -0.055, ..., -0.115, -0.095,  0.085]),\n",
       "  array([ 0.055,  0.265,  0.055, ..., -0.735, -0.575, -0.375]),\n",
       "  array([-0.295, -0.305, -0.295, ..., -0.155, -0.175, -0.155]),\n",
       "  array([-0.215, -0.185, -0.175, ..., -0.205, -0.225, -0.215]),\n",
       "  array([-0.195, -0.205, -0.185, ..., -0.225, -0.195, -0.255])],\n",
       " '19088': [array([ 0.365,  0.365,  0.355, ...,  0.795,  0.095, -0.415]),\n",
       "  array([-0.245, -0.095, -0.255, ..., -0.105, -0.125, -0.115]),\n",
       "  array([-0.115, -0.125, -0.135, ..., -0.015,  0.005, -0.035]),\n",
       "  array([-0.095, -0.175, -0.165, ..., -0.205, -0.105, -0.155]),\n",
       "  array([-0.135,  0.275,  0.805, ...,  0.015,  0.045,  0.045]),\n",
       "  array([ 0.075,  0.065,  0.075, ..., -0.105, -0.085, -0.075])],\n",
       " '19090': [array([0.165, 0.155, 0.155, ..., 0.025, 0.075, 0.095]),\n",
       "  array([ 0.165,  0.155,  0.165, ..., -0.125, -0.115, -0.105]),\n",
       "  array([-0.095, -0.075, -0.075, ..., -0.075, -0.025, -0.005]),\n",
       "  array([ 0.015,  0.035,  0.135, ..., -0.085, -0.095, -0.105]),\n",
       "  array([-0.085, -0.065, -0.095, ..., -0.105, -0.075, -0.095]),\n",
       "  array([-0.095, -0.105, -0.095, ..., -0.005,  0.025,  0.005])],\n",
       " '19093': [array([ 0.675,  0.665,  0.655, ...,  2.345, -0.005, -1.505]),\n",
       "  array([-1.065, -0.655, -0.335, ..., -0.225, -0.235, -0.225]),\n",
       "  array([-0.225, -0.215, -0.215, ..., -0.245, -0.245, -0.255]),\n",
       "  array([-0.265, -0.265, -0.275, ..., -0.225, -0.285, -0.265]),\n",
       "  array([-0.255, -0.275, -0.265, ..., -0.385, -0.385, -0.355]),\n",
       "  array([-0.385, -0.355, -0.345, ..., -0.195, -0.195, -0.185])],\n",
       " '19140': [array([ 0.255,  0.245,  0.235, ..., -0.215, -0.225, -0.215]),\n",
       "  array([-0.215, -0.245, -0.275, ..., -0.015,  0.025,  0.105]),\n",
       "  array([0.125, 0.155, 0.185, ..., 0.285, 0.315, 0.275]),\n",
       "  array([ 0.205,  0.135,  0.065, ..., -0.165, -0.175, -0.175]),\n",
       "  array([-0.175, -0.205, -0.205, ..., -0.225, -0.195, -0.225]),\n",
       "  array([-0.225, -0.205, -0.205, ...,  0.035,  0.125,  0.195])],\n",
       " '19830': [array([-0.435, -0.435, -0.415, ..., -0.045, -0.105, -0.105]),\n",
       "  array([-0.105, -0.125, -0.135, ..., -0.105, -0.115, -0.115]),\n",
       "  array([-0.115, -0.115, -0.105, ..., -0.095, -0.105, -0.105]),\n",
       "  array([-0.095, -0.075, -0.065, ..., -0.215, -0.235, -0.225]),\n",
       "  array([-0.205, -0.225, -0.235, ..., -0.055, -0.005, -0.095]),\n",
       "  array([-0.035, -0.055, -0.055, ..., -0.165, -0.165, -0.155])]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsr_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c05d9f8c-5dc6-428c-813f-67aa9666e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_nsr = \"NSR_Features_CSV\"\n",
    "os.makedirs(output_dir_nsr, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d670f267-6841-44ea-ac84-d864ffa023d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NSR subject 16265...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16265_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16265_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16265_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16265_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16265_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16265_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16272...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16272_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16272_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16272_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16272_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16272_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16272_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16273...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16273_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16273_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16273_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16273_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16273_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16273_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16420...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16420_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16420_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16420_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16420_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16420_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16420_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16483...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16483_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16483_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16483_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16483_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16483_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16483_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16539...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16539_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16539_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16539_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16539_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16539_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16539_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16773...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16773_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16773_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16773_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16773_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16773_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16773_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16786...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16786_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16786_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16786_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16786_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16786_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16786_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 16795...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16795_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16795_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16795_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16795_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16795_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\16795_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 17052...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17052_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17052_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17052_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17052_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17052_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17052_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 17453...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17453_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17453_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17453_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17453_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17453_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\17453_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 18177...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18177_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18177_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18177_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18177_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18177_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18177_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 18184...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18184_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18184_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18184_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18184_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18184_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\18184_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 19088...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19088_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19088_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19088_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19088_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19088_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19088_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 19090...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19090_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19090_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19090_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19090_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19090_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19090_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 19093...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19093_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19093_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19093_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19093_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19093_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19093_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 19140...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19140_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19140_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19140_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19140_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19140_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19140_Sixth_5_min.csv\n",
      "\n",
      "Processing NSR subject 19830...\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19830_First_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19830_Second_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19830_Third_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19830_Fourth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19830_Fifth_5_min.csv\n",
      "  -> Saved HRV features to NSR_Features_CSV\\19830_Sixth_5_min.csv\n"
     ]
    }
   ],
   "source": [
    "# Process NSR data (normal order => 1st is earliest 5 min)\n",
    "for subj_id, seg_list in nsr_segments.items():\n",
    "    print(f\"\\nProcessing NSR subject {subj_id}...\")\n",
    "    # seg_list[0] = first 5 min, seg_list[1] = second 5 min, ...\n",
    "    for i in range(6):\n",
    "        segment_label = f\"{label_map[i]}_5_min\"  # e.g. \"First_5_min\"\n",
    "        ecg_signal = seg_list[i]\n",
    "\n",
    "        denoised_signal = denoise_signal(ecg_signal, 'rbio1.5', 9, 1 , 7)\n",
    "        # 1) Detect R-peaks\n",
    "        r_peaks = r_peak_finder(denoised_signal)\n",
    "        # 2) Compute HRV\n",
    "        feats = compute_hrv_features(r_peaks, fs=128)\n",
    "        # 3) Build CSV file name, e.g. \"NSR_SubjectX_First_5_min.csv\"\n",
    "        csv_filename = f\"{subj_id}_{segment_label}.csv\"\n",
    "        csv_path = os.path.join(output_dir_nsr, csv_filename)\n",
    "        # 4) Save\n",
    "        save_hrv_to_csv(feats, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a393c0-41a3-4ee1-8cbe-f75e8453a705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
